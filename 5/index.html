<!DOCTYPE html>
<html>
<head>
    <title>Project 5: Fun with Diffusion Models </title>
    <style>
        body {
            font-family: Georgia, Times New Roman, Arial, sans-serif;
            margin: 40px;
            padding: 0px;
            background-color: #f0f8ff;
        }

        .container {
            max-width: 800px;
            margin: 40px;
            padding: 40px;
            text-align: center;
        }

        .inline-image-left {
            float: left; /* or 'right' */
            margin: 0 15px 15px 0; /* Adds space around the image */
            width: 300px; /* Optional: Control image size */
            height: auto;
        }

        .inline-image-right {
            float: right;
            margin: 0 15px 15px 0; /* Adds space around the image */
            width: 300px; /* Optional: Control image size */
            height: auto;
        }
        

        .image-row {
            display: flex;
            justify-content: center; 
            gap: 0;
        }

        .image-row img {
            width: 200px;  
            height: auto;     
            object-fit: contain;
        }

        .big-row img {
            width: 400px; 
            height: 400px;    
            padding: 0;    
            margin: 0;    
            border: none;  
            gap: 0;
        }

        .mid-row img {
            width: 100px; 
            height: 100px;    
            padding: 0;    
            margin: 0;    
            border: none;  
            gap: 5;
        }

        .text-with-image {
            display: flex;
            align-items: center; /* Aligns image and text vertically */
            gap: 15px; /* Space between image and text */
        }

        .text-with-image-left {
            display: flex;
            flex-direction: row;
            align-items: center;
            gap: 15px;
        }

        .text-with-image-left img {
            width: 300px;
            height: auto;
        }
        
        .text-with-image-right {
            display: flex;
            flex-direction: row-reverse;
            align-items: center;
            gap: 15px;
        }

        .text-with-image-right img {
            width: 200px; 
            height: auto;
        }
        
        
        .text-with-image img {
            width: 300px; /* Adjust size */
            height: auto;
        }
        
        figure {
            text-align: center;
        }
 
        .center-text {
            text-align: center; 
        }
        
    </style>
    
</head>
<body>
    <h1> Project 5: Fun with Diffusion Models </h1>
    <h2> by Maya Joiner </h2>

    
    <h2> Part 5A: The Power of Diffusion Models </h2>

    <p> AI art is becoming more and more common among short-form content, music videos, and of course, art. However, many might not understand how exactly these images are created. In this part of the project, I attempt to create different images and even optical illusions.  </p>
    <p> It's easy to "destroy" a clear image into white noise -- however, it's hard to reconstruct a real image from white noise. A more easy reconstruction process would be to reverse or undo the process of destroying an image, also known as "denoising" an image. </p>

  <h4> Prelims </h4>

  <p> For example, using HuggingFace's diffusion model package, we can create images using the prompts: </p>
    <div class="image-row big-row">
    <p><img src="media/num_inference_steps = 20 .png" /></p>
    </div>

  <p> These images were created using "inference steps" of 20. This means that the image was denoised 20 times. If we increase the number of denoising steps, we get a more "accurate" image (which looks more realistic and less AI-art looking). The drawback of increasing the denoising steps is that it takes longer. </p>

    <div class="image-row big-row">
    <p><img src="media/num_inference_steps = 100.png" /></p>
    </div>
    
    <p> The same prompts, but with <b> inference steps = 100 </b> </p>

    <div class="image-row big-row">
    <p><img src="media/num_inference = 50 step 1, 20 step 2.png" /></p>
    </div>
    
    <p> The same prompts, but with <b> inference steps = 50 </b> for step1, and <b> inference steps = 20 </b> for step2. </p>

    <p> Given this real campanile image: </p>

    <div class="image-row big-row">
    <p><img src="media/campanile.jpg" /></p>
    </div>

    <p> I perform the forward process, which is given by the equation: </p>
    <h5> sqrt(alpha) * image + sqrt(1-alpha) * epsilon </h5>
    <p> Where alpha represents a cumulative product of the amount of noise is removed each time, and epsilon represents a random value picked within a Normal distribution ~N(0, 1). I can try to blur the image to denoise it, but that doesn't work well. The best kernel size I found was k=7: </p>

    <div class="image-row big-row">
    <p><img src="media/campanile_250_500_750.png" /></p>
    <p><img src="media/1.2_denoised.png" /></p>
    </div>

    <p> Therefore, instead of simply blurring, I implement <b> one-step denoising </b>. The idea of one step denoising is that for different time steps, we estimate the amount of noise present in the noisy image. If we then subtract that estimated amount of noise from the noisy image, theoretically we should get a clean image. This works pretty well for small time steps (i.e. images that are not too noisy), but gets progressively worse as the image gets more noisy: </p>

    <div class="image-row big-row">
    <p><img src="media/1.3.png" /></p>
    </div>

    <p> To fix this issue, we denoise in many steps, also known as <b> iterative denoising </b>. In the image below, I compare iterative denoising with one-step denoising: </p>

    <div class="image-row big-row">
    <p><img src="media/1.4-1.png" /></p>
    <p><img src="media/1.4-2.png" /></p>
    </div>

    <p> These images were generated using i=10, but if I set i to 0, I can denoise random noise to make images based on a prompt (the prompt was "a high quality image"): </p>

    <div class="image-row mid-row">
    <p><img src="media/1.4-pic1.png" /></p>
    <p><img src="media/1.4-pic2.png" /></p>
    <p><img src="media/1.4-pic3.png" /></p>
    <p><img src="media/1.4-pic4.png" /></p>
    <p><img src="media/1.4-pic5.png" /></p>
    </div>

    <p> To improve the image quality even more, I use a technique called <b> Classifier-Free Guidance (CFG)</b>. The gist of this technique is that I blend a conditional image estimate (image with a prompt like "a high quality image") with an unconditional image estimate (image with no prompt) to create the final output. </p>

    <p> The equation for blending the two types of images is <b> u + gamma(c-u) </b>, where u is the unconditional noise estimate, c is the conditional noise estimate, and gamma is a constant that represents the strength of the CFG (a higher gamma value means the output will follow the conditional prompt more). </p>

    <p> Here are some images generated using CFG with a gamma value of 7: </p>
    <div class="image-row mid-row">
    <p><img src="media/1.6-pic1.png" /></p>
    <p><img src="media/1.6-pic2.png" /></p>
    <p><img src="media/1.6-pic3.png" /></p>
    <p><img src="media/1.6-pic4.png" /></p>
    <p><img src="media/1.6-pic5.png" /></p>
    </div>

    <h3> 1.7.0 </h3>

    <p> Depending on our i_start value (noise level), the accuracy of the photo (how similar it is to the test image) differs. The higher i is, the more accurate the image becomes -- by i=20, we get a campanile: </p>
    
    <div class="image-row mid-row">
    <p><img src="media/1.7-pic1.png" /></p>
    <p><img src="media/1.7-pic2.png" /></p>
    <p><img src="media/1.7-pic3.png" /></p>
    <p><img src="media/1.7-pic4.png" /></p>
    <p><img src="media/1.7-pic5.png" /></p>
    <p><img src="media/1.7-pic6.png" /></p>
    </div>

    <figure> <b> For all parts in 1.7.0: </b> noise levels (left to right): 1, 3, 5, 7, 10, 20 </figure>
    <p> I try this on two of my own images: an image of a dog, and an image of a landscape from a Pokemon game. </p>

    <div class="image-row mid-row">
    <p><img src="media/1.7.0-pic1.png" /></p>
    <p><img src="media/1.7.0-pic2.png" /></p>
    <p><img src="media/1.7.0-pic3.png" /></p>
    <p><img src="media/1.7.0.pic4.png" /></p>
    <p><img src="media/1.7.0-pic5.png" /></p>
    <p><img src="media/1.7.0-pic6.png" /></p>
    <p><img src="media/dog.jpg" /></p>
    </div>

    <div class="image-row mid-row">
    <p><img src="media/1.7.0.0-pic1.png" /></p>
    <p><img src="media/1.7.0.0-pic2.png" /></p>
    <p><img src="media/1.7.0.0-pic3.png" /></p>
    <p><img src="media/1.7.0.0-pic4.png" /></p>
    <p><img src="media/1.7.0.0-pic5.png" /></p>
    <p><img src="media/1.7.0.0-pic6.png" /></p>  
    <p><img src="media/pokemon.png" /></p>
    </div>

    <h3> 1.7.1 </h3>

    <p> This can not only be done on images I own, but can also be performed on images that exist on the internet as well as hand-drawn images. Here, I get an image of a strawberry from the web, and also draw a squid and a bird. </p>

    <div class="image-row mid-row">
    <p><img src="media/1.7.1-pic1.png" /></p>
    <p><img src="media/1.7.1-pic2.png" /></p>
    <p><img src="media/1.7.1-pic3.png" /></p>
    <p><img src="media/1.7.1-pic4.png" /></p>
    <p><img src="media/1.7.1-pic5.png" /></p>
    <p><img src="media/1.7.1-pic6.png" /></p>
    <p><img src="media/1.7.1-strawberry.jpg" /></p>
    </div>

    <figure> Image of strawberry I found on the web. </figure>

    <div class="image-row mid-row">    
    <p><img src="media/1.7.2-squid.png" /></p>
    <p><img src="media/1.7.2-squid-drawing.png" /></p>
    <p><img src="media/1.7.2-processed.png" /></p>    
    </div>

    <figure> I tried to draw a squid (middle photo) based on the photo (left) I found on the web. Because the editing software had a rectangular UI, my image got squashed when cropping it into a square so that I could perform CFG on it (right). </figure>

    <div class="image-row mid-row">    
    <p><img src="media/1.7.2-pic1.png" /></p>
    <p><img src="media/1.7.2-pic2.png" /></p>
    <p><img src="media/1.7.2-pic3.png" /></p>
    <p><img src="media/1.7.2-pic4.png" /></p>
    <p><img src="media/1.7.2-pic5.png" /></p>
    <p><img src="media/1.7.2-pic6.png" /></p>
    </div>

    <figure> Weird that I get two humans at i=10... </figure>

    <div class="image-row mid-row">    
    <p><img src="media/1.7.2.2-bird.jpg" /></p>
    <p><img src="media/1.7.2.2-bird-drawing.png" /></p>
    <p><img src="media/1.7.2.2-bird-processed.png" /></p>    
    </div>

    <figure> I tried to draw my favorite bird (middle photo) based on the photo (left) of the bird I found on the web. </figure>
    

    <div class="image-row mid-row">    
    <p><img src="media/1.7.2.2-pic1.png" /></p>
    <p><img src="media/1.7.2.2-pic2.png" /></p>
    <p><img src="media/1.7.2.2-pic3.png" /></p>
    <p><img src="media/1.7.2.2-pic4.png" /></p>
    <p><img src="media/1.7.2.2-pic5.png" /></p>
    <p><img src="media/1.7.2.2-pic6.png" /></p>
    </div>

    <figure> One funny thing about this sequence is that at i=7 and i=10, I get images of a girl wearing clothes of the same color as the bird. The image only changes into that of a bird at i=20. </figure>

    <h3> 1.7.2 </h3>

    <p> We can also use CFG to perform inpainting. If I have a mask of any shape, I can replace just parts of the test image where the mask is white (equal to 1.0). For example, I define a square mask on the campanile: </p>

    <div class="image-row mid-row">    
    <p><img src="media/1.7-pic6.png" /></p>
    <p><img src="media/1.7.3-2.png" /></p>
    </div>

    <p> As you can see in the right image, just the square part is inpainted, so just the contents of the square are edited/created using CFG. The top of the campanile, when inpainted with a square mask on top, doesn't look like the campanile anymore. I do this with other images too: </p>
    <div class="image-row mid-row">    
    <p><img src="media/dog.jpg" /></p>
    <p><img src="media/masked_doggo2.png"/></p>
    </div>

    <figure> When putting a square mask on the terrier's face (right), the inpainting makes the dog's face look like a chihuahua. </figure>

    <div class="image-row mid-row">    
    <p><img src="media/pokemon.png" /></p>
    <p><img src="media/new_pokemon.png"/></p>
    </div>

    <figure> This time I use a vertical mask, where the left side is the real image and the right side is inpainted. </figure>
    
    <h3> 1.7.3 </h3>

    <p> Here, I use <b> text-conditioned translation </b>. For part 1.7.1-1.7.2, I used just the prompt "a high quality image" so there was freedom among what kind of images were generated during each timestep (ex. a photo of a girl generated before the bird image in 1.7.2), but if we instead provide a prompt like "a rocket ship", then we can generate photos that look like the prompt before reaching the final image in the last timestep. Here, you can see that the images during the intermediate timesteps look like a rocket ship before ultimately converging to my Pokemon image from the previous parts: </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.7.3.1-pic1.png" /></p>
    <p><img src="media/1.7.3.1-pic2.png" /></p>
    <p><img src="media/1.7.3.1-pic3.png" /></p>
    <p><img src="media/1.7.3.1-pic4.png" /></p>
    <p><img src="media/1.7.3.1-pic5.png" /></p>
    <p><img src="media/1.7.3.1-pic6.png" /></p>
    </div>

    <p> I try this on the campanile image, with the same rocket ship prompt: </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.7.3.2-pic1.png" /></p>
    <p><img src="media/1.7.3.2-pic2.png" /></p>
    <p><img src="media/1.7.3.2-pic3.png" /></p>
    <p><img src="media/1.7.3.2-pic4.png" /></p>
    <p><img src="media/1.7.3.2-pic5.png" /></p>
    <p><img src="media/1.7.3.2-pic6.png" /></p>
    </div>

    <p> I can also try different prompts, like "a photo of a hipster barista" converging to my dog image: </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.7.3.3-pic1.png" /></p>
    <p><img src="media/1.7.3.3-pic2.png" /></p>
    <p><img src="media/1.7.3.3-pic3.png" /></p>
    <p><img src="media/1.7.3.3-pic4.png" /></p>
    <p><img src="media/1.7.3.3-pic5.png" /></p>
    <p><img src="media/1.7.3.3-pic6.png" /></p>
    </div>

    <h3> 1.8 </h3>

    <p> I could also use unets to create images anagrams. These are images that look like one thing, but when flipped look like something else. In order to do this, I first generate an image with one prompt using a unet and call that e1. Then. I flip the original image and generate another image with the other prompt using the flipped image. I flip that to get e2. To get the final noise estimate e, I average e1 and e2 and add variance (similar to the CFG step from 1.6). Here, I create an image that looks like an old man (left) but looks like people around a campfire when the image is flipped (right): </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.8-1.png" /></p>
    <p><img src="media/1.8-1-flip.png" /></p>
    </div>

    <p> Here's another example: </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.8-2.png" /></p>
    <p><img src="media/1.8-2-flip.png" /></p>
    </div>

    <p> Now I use other prompts: a snowy mountain village from the right side up, but a skull when flipped: </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.8-3.png" /></p>
    <p><img src="media/1.8-3-flip.png" /></p>
    </div>

    <p> My final example is that of a dog, but a human when flipped: </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.8-4.png" /></p>
    <p><img src="media/1.8-4-flip.png" /></p>
    </div>

    <h3> 1.9 </h3>

    <p> I can also use a similar code sequence to create hybrid images, which are images that look like one thing from close up but look like another thing from far away. This can be done using lowpass and highpass filters, just like from project 2. The first two images look like a waterfall from up close, but look like a skull from far away. The image in the middle looks like a snowy village from up close, but a rocket ship from far away. The two images on the right look like people at a campfire from up close, but a man from far away. </p>
    <div class="image-row mid-row">    
    <p><img src="media/1.9-1.png" /></p>
    <p><img src="media/1.9-2.png" /></p>
    <p><img src="media/1.9-3.png" /></p>
    <p><img src="media/1.9-4.png" /></p>
    <p><img src="media/1.9-5.png" /></p>
    </div>

    <h3> Part B </h3>
    <p> I unfortunately didn't have enough time to finish part B because of a business trip, but I was able to do part of the first question, which is to create a noisy image for the digits. I then tried to train the model, but one trouble I had was that torch.concat kept changing the shape of my image to the shape described in the diagram on the spec. </p>

    <div class="image-row mid-row">    
    <p><img src="media/5b-1.1.png" /></p>
    </div>


</body>
</html>
